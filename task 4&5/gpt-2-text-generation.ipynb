{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os,glob\nimport os.path\n\npath = \"../input/inputtext/text_outputs\"\nfiles = []\next='*.txt'\nfor dirpath, dirnames, filenames in os.walk(path):\n    files += glob.glob(os.path.join(dirpath, ext))\nprint(files)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-22T11:41:47.393036Z","iopub.execute_input":"2022-06-22T11:41:47.393517Z","iopub.status.idle":"2022-06-22T11:41:47.46853Z","shell.execute_reply.started":"2022-06-22T11:41:47.39347Z","shell.execute_reply":"2022-06-22T11:41:47.46755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"/kaggle/working/all_texts.txt\", \"w\") as outfile:\n    for filename in files:\n        with open(filename) as infile:\n            contents = infile.read()\n            outfile.write(contents)","metadata":{"execution":{"iopub.status.busy":"2022-06-22T11:41:52.610243Z","iopub.execute_input":"2022-06-22T11:41:52.610934Z","iopub.status.idle":"2022-06-22T11:41:53.43097Z","shell.execute_reply.started":"2022-06-22T11:41:52.610897Z","shell.execute_reply":"2022-06-22T11:41:53.429869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install gpt-2-simple","metadata":{"execution":{"iopub.status.busy":"2022-06-22T11:41:57.017536Z","iopub.execute_input":"2022-06-22T11:41:57.017893Z","iopub.status.idle":"2022-06-22T11:42:29.803299Z","shell.execute_reply.started":"2022-06-22T11:41:57.017864Z","shell.execute_reply":"2022-06-22T11:42:29.802176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gpt_2_simple as gpt2\nimport os\nimport requests\n\nmodel_name = \"124M\"\nif not os.path.isdir(os.path.join(\"models\", model_name)):\n    print(f\"Downloading {model_name} model...\")\n    gpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /models/124M/\n\npdf_file=open(\"/kaggle/working/all_texts.txt\",'rt')\ncontent=pdf_file.read()\n\nfile_name = \"texts_pdf.txt\"\nif not os.path.isfile(file_name):\n    with open(file_name, 'w') as f2:\n        f2.write(content)\n\nsess = gpt2.start_tf_sess()\ngpt2.finetune(sess,\n              file_name,\n              model_name=model_name,\n              steps=100)   # steps is max number of training steps\n\ngpt2.generate(sess)","metadata":{"execution":{"iopub.status.busy":"2022-06-22T11:42:29.805974Z","iopub.execute_input":"2022-06-22T11:42:29.806588Z","iopub.status.idle":"2022-06-22T11:46:49.026469Z","shell.execute_reply.started":"2022-06-22T11:42:29.806547Z","shell.execute_reply":"2022-06-22T11:46:49.025452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.mkdir('generation_texts')","metadata":{"execution":{"iopub.status.busy":"2022-06-22T11:46:49.028349Z","iopub.execute_input":"2022-06-22T11:46:49.028915Z","iopub.status.idle":"2022-06-22T11:46:49.033974Z","shell.execute_reply.started":"2022-06-22T11:46:49.028875Z","shell.execute_reply":"2022-06-22T11:46:49.033043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generation_text_50_100=[]\nfor i in range(150,200):\n    generation_text_50_100.append(gpt2.generate(sess, return_as_list=True))\n    print('finish '+str(i))\nfor i in range(0,50):\n    path='generation_texts/fake_article_'+str(i)+'.txt'\n    with open(path,'w',encoding='utf-8') as f:\n        print(generation_text_50_100[i][0])\n        f.write(generation_text_50_100[i][0])","metadata":{"execution":{"iopub.status.busy":"2022-06-22T11:50:11.340518Z","iopub.execute_input":"2022-06-22T11:50:11.341227Z","iopub.status.idle":"2022-06-22T11:50:23.69797Z","shell.execute_reply.started":"2022-06-22T11:50:11.341188Z","shell.execute_reply":"2022-06-22T11:50:23.69682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generation_text_50_100[0][0]","metadata":{"execution":{"iopub.status.busy":"2022-06-22T11:48:40.257519Z","iopub.execute_input":"2022-06-22T11:48:40.257861Z","iopub.status.idle":"2022-06-22T11:48:40.265684Z","shell.execute_reply.started":"2022-06-22T11:48:40.257832Z","shell.execute_reply":"2022-06-22T11:48:40.264539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}